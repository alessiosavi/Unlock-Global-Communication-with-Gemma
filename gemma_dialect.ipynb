{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77febf8d4f3740cb",
   "metadata": {},
   "source": [
    "# DialectGemma, finetune Gemma in order to talk like your grandpa!\n",
    "\n",
    "### Goals\n",
    "The goal of this project is to be able to basically translate any dialect to the original \"base\" languages, without dealing with a complex data pipeline.\n",
    "Due to this, I've built a straightforward pipeline that leverages \"lemmatization and contextual similarity\" on the text dialect dataset in order to enhance the translation of the text. The concept idea, is to translate something that it's basically not understandable by any existing model, to a simplified version of the text, passing through the lemmatization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6841f76e445d925f",
   "metadata": {},
   "source": [
    "\n",
    "import gc\n",
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "from typing import *\n",
    "\n",
    "# langdetect is used to detect the most similar language of the dialect, by this way we can enhance the translation process\n",
    "import langdetect\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk.translate.bleu_score as bleu\n",
    "import nltk.translate.gleu_score as gleu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import seaborn as sns\n",
    "# spacy is used to perform the lemmatization\n",
    "import spacy\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "# Some dialect has phonetic symbols, with `unidecode` library we aim to clean these phonetics symbols\n",
    "from unidecode import unidecode\n",
    "from unsloth import FastLanguageModel, load_correct_tokenizer\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "max_seq_length = 1024\n",
    "min_seq_length = 0\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_YOUR_TOKEN_HERE\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "def _collect():\n",
    "    x = 0\n",
    "    for i in range(3):\n",
    "        x += gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "batch_regex = re.compile(r\"(?<=[.!?\\n])\\s+\")\n",
    "punctuation_set = set(string.punctuation)\n",
    "# We want to build a text dataset, sometimes wikipedia (especially for dialect) tends to have a list of dates.\n",
    "# This regexp aims to find (and count) the numbers of 'date/numbers' in the text. We want to remove these examples.\n",
    "number_regex = re.compile(r\"\\d*\\.\\d+|\\d+\", re.MULTILINE)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# Due to my memory limitation, I batch each wikipedia page in a set of fixed number of words.\n",
    "#  The last sentence of the first batch is the first sentence of the next batch.\n",
    "#  Example: 'This is an example sentence. This sentence has to be separated. This, because it's too much to be handled by my GPU!'\n",
    "#  Output:\n",
    "#   BATCH 1: 'This is an example sentence. This sentence has to be separated.'\n",
    "#   BATCH 2: 'This sentence has to be separated. This, because it's too much to be handled by my GPU!'\n",
    "# The idea here is to have \"continuity\" of the context.\n",
    "def batch_text_with_overlap(text: str, max_words=256) -> List[str]:\n",
    "    \"\"\"Batches text into chunks with a maximum number of words, allowing overlap between batches.\n",
    "\n",
    "    Args:\n",
    "        text: The input text string.\n",
    "        max_words: The maximum number of words per batch.\n",
    "\n",
    "    Returns:\n",
    "        :return:A list of string batches.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = batch_regex.split(text)\n",
    "\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_word_count = len(sentence.split())\n",
    "\n",
    "        if current_word_count + sentence_word_count <= max_words:\n",
    "            # Add sentence to current batch\n",
    "            current_batch.append(sentence)\n",
    "            current_word_count += sentence_word_count\n",
    "        else:\n",
    "            # The current batch is ready, add it to batches\n",
    "            if current_batch:\n",
    "                batches.append(\" \".join(current_batch).strip())\n",
    "                # Start a new batch, including the last sentence of the previous batch\n",
    "                current_batch = [sentence]\n",
    "                current_word_count = sentence_word_count\n",
    "                # Overlap: add the last sentence of the previous batch to the new batch\n",
    "                if i > 0:\n",
    "                    current_batch.insert(0, sentences[i - 1])\n",
    "                    current_word_count += len(sentences[i - 1].split())\n",
    "            else:\n",
    "                # Handle cases where a single sentence is longer than max_words\n",
    "                batches.append(sentence.strip())\n",
    "                current_batch = []\n",
    "                current_word_count = 0\n",
    "\n",
    "    # Add the last batch if it's not empty\n",
    "    if current_batch:\n",
    "        batches.append(\" \".join(current_batch).strip())\n",
    "    return batches\n",
    "\n",
    "\n",
    "def find_outliers(data, n=2):\n",
    "    \"\"\"\n",
    "    Identifies outliers in a given Pandas Series based on the Z-score.\n",
    "    This method leverages Z-score to clean \"dirty\" entries.\n",
    "\n",
    "       Args:\n",
    "           :param data: A Pandas Series containing numerical data.\n",
    "           :param n: The number of standard deviations to use as a threshold for outlier detection.\n",
    "\n",
    "       Returns:\n",
    "          :return: A boolean Pandas Series where True indicates an outlier and False indicates a non-outlier.\n",
    "    \"\"\"\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    data_zscore = (data - mean) / std\n",
    "\n",
    "    # Identify outliers (e.g., |Z| > 3)\n",
    "    mask = abs(data_zscore) > n\n",
    "    return mask\n",
    "\n",
    "\n",
    "def count_punctuation_and_numbers(text: str):\n",
    "    \"\"\"\n",
    "    Counts the number of punctuation marks, numbers, and newline characters in a given string.\n",
    "    This method is used in combination with `find_outliers` to clean \"dirty\" entries.\n",
    "\n",
    "       Args:\n",
    "           :param text: The input string.\n",
    "\n",
    "       Returns:\n",
    "           :return: A dictionary containing the counts of punctuation marks, numbers, and newline characters, or a dictionary with zero counts if the input string is empty.\n",
    "       \"\"\"\n",
    "    if not text or len(text) == 0:\n",
    "        return {\"puncts\": 0, \"numbers\": 0, \"new_lines\": 0}\n",
    "\n",
    "    punct_count = sum(1 for char in text if char in punctuation_set)\n",
    "    number_count = len(number_regex.findall(text))\n",
    "    new_lines = text.count(\"\\n\")\n",
    "\n",
    "    return {\"puncts\": punct_count, \"numbers\": number_count, \"new_lines\": new_lines}\n",
    "\n",
    "\n",
    "def translate(text, target_language='Italian', model='meta-llama-3.1-70B-bnb-4bit'):\n",
    "    \"\"\"\n",
    "    This method translates a given text into its corresponding language model.\n",
    "    I've used a local cluster, using `unsloth/Meta-Llama-3.1-70B-bnb-4bit`.\n",
    "    The model servers exposes a JSON over HTTP API that follows the [OpenAI spec](https://platform.openai.com/docs/api-reference/chat).\n",
    "    This code is omitted cause is only infra-related stuff\n",
    "    Args:\n",
    "        :param text: The text to be translated.\n",
    "        :param target_language: The language name of the target language.\n",
    "        :param model: The LLM to use.\n",
    "    Return:\n",
    "        :return: Translated text.\n",
    "    \"\"\"\n",
    "    url = \"YOUR_CLUSTER_URL\"\n",
    "    prompt = f\"You are a translator that don't add any sentences that are not written in the input text. Don't add also any comments, just translate the text into plain {target_language}.\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    resp = requests.post(\n",
    "        url,\n",
    "        stream=False,\n",
    "        timeout=(30, 300),\n",
    "        json={\n",
    "            \"model\": \"local\",\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "        },\n",
    "        headers={\n",
    "            \"x-service-id\": model\n",
    "        }\n",
    "    )\n",
    "    resp.raise_for_status()\n",
    "    return resp\n",
    "\n",
    "\n",
    "def translate_local(text, target_language=\"it\", model_name=\"Helsinki-NLP/opus-mt-fr-en\"):\n",
    "    \"\"\"\n",
    "    Translates text from one language to another using a specified LLM.\n",
    "    NOTE: This method is not tested, is saved as reference in order to play/debug the solution without having a local GPU cluster\n",
    "\n",
    "    Args:\n",
    "        :param text: The text to be translated.\n",
    "        :param target_language: The target language code (e.g., \"en\" for English, \"fr\" for French, \"es\" for Spanish).\n",
    "        :param model_name: The name of the translation model from the Hugging Face Model Hub.\n",
    "\n",
    "    Returns:\n",
    "        :return: The translated text, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        translator = pipeline(\"translation\", model=model_name)\n",
    "        translated_text = translator(text, target_lang=target_language)[0]['translation_text']\n",
    "        return translated_text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during translation: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_dataset(dataset: Dict[str, List[str]], n: int = 2) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Cleans a dataset by removing entries that are outliers based on punctuation and number counts.\n",
    "\n",
    "    Args:\n",
    "        :param: dataset: A dictionary where keys are identifiers and values are lists of strings.\n",
    "        :param: n: The number of standard deviations to use as a threshold for outlier detection.\n",
    "\n",
    "    Returns:\n",
    "        :return: A new dictionary containing only the entries that are not considered outliers.\n",
    "    \"\"\"\n",
    "    puncts = {}\n",
    "    for k, v in dataset.items():\n",
    "        puncts[k] = count_punctuation_and_numbers(\"\\n\".join(v))\n",
    "    puncts = pd.DataFrame(puncts).T.reset_index()\n",
    "    mask1 = find_outliers(puncts[\"puncts\"], n=n)\n",
    "    mask2 = find_outliers(puncts[\"numbers\"], n=n)\n",
    "    mask = mask1 | mask2\n",
    "    keys = set(puncts[mask][\"index\"].to_list())\n",
    "    return {k: v for k, v in dataset.items() if k not in keys}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c7ac0dfcdf54349",
   "metadata": {},
   "source": [
    "# Chapter 0: Raw dataset creation.\n",
    "Leverage lemmatization and semantic similarity in order to translate raw dialect text into a plain language.\n",
    "### Some notes on the following process\n",
    "In this notebook, I aim to let the process be straightforward. This is because I want to reduce the time spent by the reviewer to understand the idea, and be straightforward to use.\n",
    "If you want to use some sort of automation, batching, etc., please refer to the [repository](https://github.com/alessiosavi/Unlock-Global-Communication-with-Gemma) where the code is modularized into libraries instead of a single monolithic Notebook.\n",
    "In order to demonstrate the great capabilities of this approach (and the related works that can be done), I've used [interlingua](https://en.wikipedia.org/wiki/Interlingua) as base dataset for this example.\n",
    "\n",
    "Please just have a look at this process and jump to the real part of the dataset creation, **_Chapter 1_**."
   ]
  },
  {
   "cell_type": "code",
   "id": "c74b07a90849e806",
   "metadata": {},
   "source": [
    "def load_raw_data(language: str, keys: Union[List[str], None] = None):\n",
    "    \"\"\"Loads raw Wikipedia data for a given language, applying filtering and preprocessing.\n",
    "\n",
    "    Args:\n",
    "        :param: language: The language code (e.g., \"en\", \"it\").\n",
    "        :param: keys: An optional list of titles to filter the dataset. If None, all titles are used.\n",
    "\n",
    "    Returns:\n",
    "        :return: A dictionary where keys are titles and values are lists of text batches (with overlap).\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    df = load_dataset(\"wikimedia/wikipedia\", f\"20231101.{language}\", num_proc=6)[\"train\"].to_pandas()\n",
    "    if keys is not None:\n",
    "        df = df[df[\"title\"].isin(keys)]\n",
    "    df[\"size\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "    df = df[(df[\"size\"] >= 128) & (df[\"size\"] <= 1500)]\n",
    "    df[\"text\"] = df[\"text\"].str.strip()\n",
    "    mask1 = df[\"text\"].str.contains(\"{{\", regex=False) | df[\"text\"].str.contains(\"}}\", regex=False)\n",
    "    mask2 = df[\"text\"].str.contains(\"[[\", regex=False) | df[\"text\"].str.contains(\"]]\", regex=False)\n",
    "    mask3 = df[\"text\"].str.contains(\"{|\", regex=False) | df[\"text\"].str.contains(\"|}\", regex=False)\n",
    "    df = df[~(mask1 | mask2 | mask3)].sample(frac=1, random_state=seed)\n",
    "    ds_overlap = defaultdict(list)\n",
    "    df[[\"title\", \"text\"]].apply(\n",
    "        lambda x: ds_overlap[x[\"title\"]].extend(batch_text_with_overlap(x[\"text\"])),\n",
    "        axis=1,\n",
    "    )\n",
    "    assert len(ds_overlap.keys()) == len(df)\n",
    "\n",
    "    ds_overlap = clean_dataset(ds_overlap, 2)\n",
    "    print(f\"Len after removing puncts/numbers: {len(ds_overlap)}\")\n",
    "    return ds_overlap\n",
    "\n",
    "\n",
    "# Load the raw dataset from Wikipedia, `ia` is the dataset for `interlingua`\n",
    "ds_overlap = load_raw_data(language='ia')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "233a82450ede731e",
   "metadata": {},
   "source": [
    "### Preprocessing and Lemmatization\n",
    "In this cell we are going to define how the raw text will be processed.\n",
    "- Use the `unidecode` method to \"translate\" phonetic symbols\n",
    "- Detect the most similar language\n",
    "- Use the appropriated spacy model to lemmatize the text"
   ]
  },
  {
   "cell_type": "code",
   "id": "4c29ad838180f023",
   "metadata": {},
   "source": [
    "def detect_language(text: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        :param text: Input text language\n",
    "    Returns:\n",
    "        :return: 2-digit of the language code (e.g., \"en\" for English)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lang = langdetect.detect(text)\n",
    "        return lang\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting language: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_text(text: str, nlp: spacy.Language):\n",
    "    \"\"\"\n",
    "    Preprocesses text using a given SpaCy NLP model for lemmatization.\n",
    "\n",
    "    Args:\n",
    "        :param: text: The input text string.\n",
    "        :param: nlp: The SpaCy NLP model to use for processing.\n",
    "\n",
    "    Returns:\n",
    "        :return: The lemmatized text string. Returns the original text if an error occurs during preprocessing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc]).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing: {e}\")\n",
    "        return text  # Return the original text if preprocessing fails\n",
    "\n",
    "\n",
    "nlps: Dict[str, spacy.Language] = {}\n",
    "\n",
    "\n",
    "def lemmatize_data(text: str, force_language: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "     Lemmatizes the input text using SpaCy, handling language detection and model loading.\n",
    "\n",
    "    Args:\n",
    "        :param: text: The input text string.\n",
    "        :param: force_language: An optional language code to force lemmatization in a specific language. If None, the language is detected automatically.\n",
    "\n",
    "    Returns:\n",
    "        :return: The lemmatized text string (or an exception if `langdetext` is not able to understand the language code).\n",
    "    \"\"\"\n",
    "    decoded_text = unidecode(text)\n",
    "    language: str = detect_language(decoded_text) if not force_language else force_language\n",
    "    if language in nlps:\n",
    "        nlp = nlps[language]\n",
    "    else:\n",
    "        if language.startswith('en'):\n",
    "            nlp = spacy.load(f\"{language}_core_web_md\")\n",
    "        else:\n",
    "            nlp = spacy.load(f\"{language}_core_news_md\")\n",
    "        nlps[language] = nlp\n",
    "    return preprocess_text(decoded_text, nlp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63d97add830ec15e",
   "metadata": {},
   "source": [
    "# Here is an example of the lemmatization process, leveraging the straightforward pipeline above\n",
    "for vs in ds_overlap.values():\n",
    "    for v in vs:\n",
    "        print(\"Before\\n\", v, '\\n', \"---\" * 30)\n",
    "        print(\"After\\n\", lemmatize_data(v), '\\n', \"---\" * 30)\n",
    "    break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7624896160cad0f6",
   "metadata": {},
   "source": [
    "# Dirty hack to avoid losing the already translated dataset.\n",
    "if \"ds_overlap_translated\" not in globals():\n",
    "    ds_overlap_translated = defaultdict(list)\n",
    "    # Load the latest file saved, instead of restart from 0\n",
    "    files = glob(\"data/ds_overlap_translated_nap_lemma_it*\")\n",
    "    if len(files) > 0:\n",
    "        files.sort(key=lambda x: os.path.getmtime(x))\n",
    "        ds_overlap_translated = defaultdict(list, json.load(gzip.open(files[-1])))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f3ebb034951d30c2",
   "metadata": {},
   "source": [
    "#### Translation phase\n",
    "This process is written in a \"hackable\" way to let other people be able to play with it. You can think about these few lines as a backbone that should be adjusted for your scope."
   ]
  },
  {
   "cell_type": "code",
   "id": "6fe7831849949d82",
   "metadata": {},
   "source": [
    "print(f\"Len of already translated data: {len(ds_overlap_translated)}\")\n",
    "n = 0\n",
    "for k, vs in tqdm(ds_overlap.items(), total=len(ds_overlap), position=0):\n",
    "    # Avoid translating the already managed topics\n",
    "    if k in ds_overlap_translated and len(vs) == len(ds_overlap_translated[k]):\n",
    "        continue\n",
    "    for idx, v in enumerate(tqdm(vs, leave=False, position=1, desc=k)):\n",
    "        if (idx + 1) <= len(ds_overlap_translated[k]):\n",
    "            continue\n",
    "        try:\n",
    "            lemmatized_v = lemmatize_data(v)\n",
    "            res = translate(lemmatized_v)\n",
    "            response = (\n",
    "                res.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                # This cleaning is related to the LLAMA model used.\n",
    "                .removeprefix(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "                .strip()\n",
    "            )\n",
    "            ds_overlap_translated[k].append(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Missing idx: {idx} of {k} | Error: {e}\")\n",
    "            del ds_overlap_translated[k]\n",
    "            break\n",
    "    n += 1\n",
    "    if n % 5 == 0:\n",
    "        # Save the dataset every 5 wikipedia webpages translated\n",
    "        fname = f\"data/ds_overlap_translated_nap_lemma_it_{n}_{len(ds_overlap_translated)}.json\"\n",
    "        with gzip.open(fname, \"wt\") as zipfile:\n",
    "            json.dump(ds_overlap_translated, zipfile)\n",
    "# Save the last version of the dataset\n",
    "json.dump(ds_overlap_translated, open(\"ds_overlap_translated_ia_lemma_it.json\", \"wt\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d65035472a460863",
   "metadata": {},
   "source": [
    "# Chapter 1: Creating a clean dataset from raw translation to fine-tuning Gemma2\n",
    "Here we demonstrate how to get the best out of our raw data.\n",
    "We are going to create two different types of dataset:\n",
    "  - Translate this stuff\n",
    "  - Describe this stuff\n",
    "\n",
    "Our final goal is to have a model that can translate the dialect to an eligible language, but we train also to be able to \"describe\" a given topic in the dialect language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462931fd9806b59e",
   "metadata": {},
   "source": [
    "### Instruction template definition\n",
    "We use the standard template of Gemma2 for conversation as stated by [Gemma2 formatting docs](https://ai.google.dev/gemma/docs/formatting)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2259cb93f40ad35",
   "metadata": {},
   "source": [
    "instruction_translate = \"Provide a punctual translation of the following text from {} to {}, without any comments, explanations or interpretations.\"\n",
    "instruction_describe = \"Describe the following topic in the following language: {}.\"\n",
    "\n",
    "alpaca_prompt_template = \"\"\"<start_of_turn>user\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "### Response:\n",
    "{}\n",
    "<end_of_turn>\"\"\"\n",
    "_collect()\n",
    "print(\n",
    "    alpaca_prompt_template.format(\n",
    "        instruction_translate.format(\"Italian\", \"English\"), \"INPUT\", \"OUTPUT\"\n",
    "    )\n",
    ")\n",
    "print(\"---\" * 30)\n",
    "print(\n",
    "    alpaca_prompt_template.format(\n",
    "        instruction_describe.format(\"Italian\"), \"INPUT\", \"OUTPUT\"\n",
    "    )\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5173c46acbdb8fe7",
   "metadata": {},
   "source": [
    "\n",
    "def remove_content_by_target(d: Dict[str, List[str]], targets: List[str], lower=True):\n",
    "    \"\"\"Removes content from a dictionary based on the presence of specified target strings.\n",
    "\n",
    "    Args:\n",
    "        :param: d: A dictionary where keys are identifiers and values are lists of strings.\n",
    "        :param: targets: A list of target strings to search for within the content.\n",
    "        :param: lower: A boolean indicating whether to convert the content to lowercase before searching for targets.\n",
    "\n",
    "    Returns:\n",
    "        :return: A new dictionary containing only the content that does not contain any of the target strings.\n",
    "    \"\"\"\n",
    "    data = defaultdict(list)\n",
    "    for k, vs in d.items():\n",
    "        for v in vs:\n",
    "            _v = copy(v)\n",
    "            if lower:\n",
    "                _v = _v.lower()\n",
    "            if not any(target in _v for target in targets):\n",
    "                data[k].append(v)\n",
    "    return data\n",
    "\n",
    "\n",
    "def find_topic_by_target(d: Dict[str, List[str]], targets: List[str], lower=True):\n",
    "    \"\"\"Return the entries from a dictionary based on the presence of specified target strings.\n",
    "\n",
    "    Args:\n",
    "        :param: d: A dictionary where keys are identifiers and values are lists of strings.\n",
    "        :param: targets: A list of target strings to search for within the content.\n",
    "        :param: lower: A boolean indicating whether to convert the content to lowercase before searching for targets.\n",
    "\n",
    "    Returns:\n",
    "        :return: The keys of the input dictionary that contains any of the target strings.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for k, vs in d.items():\n",
    "        text = \"\\n\".join(vs)\n",
    "        if lower:\n",
    "            text = text.lower()\n",
    "        if any(target in text for target in targets):\n",
    "            keys.append(k)\n",
    "    return keys\n",
    "\n",
    "\n",
    "def remove_similar_documents(df, text_column, similarity_threshold=0.2):\n",
    "    \"\"\"Removes similar documents from a DataFrame based on TF-IDF and cosine similarity.\n",
    "\n",
    "     Args:\n",
    "         :param: df: The input Pandas DataFrame.\n",
    "         :param: text_column: The name of the column containing the text data.\n",
    "         :param: similarity_threshold: The threshold above which two documents are considered similar.\n",
    "\n",
    "     Returns:\n",
    "         :return: A new Pandas DataFrame with similar documents removed. Returns the original DataFrame if it's empty.\n",
    "     \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # 1. TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[text_column])\n",
    "\n",
    "    # 2. Cosine Similarity Calculation\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # 3. Identify Documents to Remove\n",
    "    to_remove = set()\n",
    "    num_docs = len(df)\n",
    "    for i in range(num_docs):\n",
    "        for j in range(i + 1, num_docs):\n",
    "            if similarity_matrix[i, j] > similarity_threshold:\n",
    "                # Prioritize keeping the earlier document encountered in the DataFrame\n",
    "                to_remove.add(j)\n",
    "\n",
    "    # 4. Create a new DataFrame\n",
    "    indices_to_keep = list(set(range(num_docs)) - to_remove)\n",
    "    filtered_df = df.iloc[list(indices_to_keep)].copy()\n",
    "    duplicated_df = df.iloc[list(to_remove)]\n",
    "\n",
    "    return filtered_df, duplicated_df\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# config = {\n",
    "#     \"starting_language\": \"sicilian\",\n",
    "#     \"wikilanguage\": 'scn',\n",
    "#\n",
    "#     \"italian\": \"raw_datasets/ds_overlap_translated_scn_lemma_it.json.gz\",\n",
    "#     \"english\": \"raw_datasets/ds_overlap_translated_scn_lemma_en.json.gz\"\n",
    "# }\n",
    "#\n",
    "# config = {\n",
    "#     \"starting_language\": \"napolitan\",\n",
    "#     \"wikilanguage\": 'nap',\n",
    "#\n",
    "#     \"italian\": \"raw_datasets/ds_overlap_translated_nap_lemma_it.json.gz\",\n",
    "#     \"english\": \"raw_datasets/ds_overlap_translated_nap_lemma_en.json.gz\"\n",
    "# }\n",
    "# config = {\n",
    "#     \"starting_language\": \"venetian\",\n",
    "#     \"wikilanguage\": 'vec',\n",
    "#\n",
    "#     \"italian\": \"raw_datasets/ds_overlap_translated_vec_lemma_it.json.gz\",\n",
    "#     \"english\": \"raw_datasets/ds_overlap_translated_vec_lemma_en.json.gz\"\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    \"starting_language\": \"interlingua\",\n",
    "    \"wikilanguage\": 'ia',\n",
    "\n",
    "    \"italian\": \"raw_datasets/ds_overlap_translated_ia_lemma_it.json.gz\",\n",
    "    \"english\": \"raw_datasets/ds_overlap_translated_ia_lemma_en.json.gz\"\n",
    "}\n",
    "\n",
    "it_fname = config['italian']\n",
    "en_fname = config['english']\n",
    "starting_language = config['starting_language']\n",
    "wikilanguage = config['wikilanguage']\n",
    "ds_overlap_it: Dict[str, List[str]] = json.load(gzip.open(it_fname))\n",
    "ds_overlap_en: Dict[str, List[str]] = json.load(gzip.open(en_fname))\n",
    "\n",
    "\n",
    "def aggregate_dataset(raw_base: Dict[str, List[str]], raw_translated: Dict[str, List[str]]) -> Dict[\n",
    "    str, List[Dict[str, List[str]]]]:\n",
    "    \"\"\"Aggregates two raw datasets (base and translated) into a single structured dataset.\n",
    "\n",
    "    Args:\n",
    "        :param: raw_base: A dictionary where keys are topics and values are lists of strings (base data).\n",
    "        :param: raw_translated: A dictionary where keys are topics and values are lists of strings (translated data).\n",
    "\n",
    "    Returns:\n",
    "        :return: A dictionary where keys are topics and values are lists of dictionaries.\n",
    "            Each inner dictionary contains the \"original\" and \"translated\" strings for a given data point.\n",
    "    \"\"\"\n",
    "    ds = {}\n",
    "    for topic in raw_translated:\n",
    "        base_data = raw_base[topic]\n",
    "        translated_data = raw_translated[topic]\n",
    "        d = []\n",
    "        for v1, v2 in zip(base_data, translated_data):\n",
    "            d.append({\"original\": v1, \"translated\": v2})\n",
    "        ds[topic] = d\n",
    "    return ds\n",
    "\n",
    "\n",
    "def filter_dataset(language, ds_translated, n=2, ds_base=None):\n",
    "    \"\"\"Filters and cleans a translated dataset based on a base dataset.\n",
    "\n",
    "    Args:\n",
    "        :param: language: The language of the dataset.\n",
    "        :param: ds_translate: A dictionary representing the translated dataset (topic -> list of strings).\n",
    "        :param: n: A threshold for outlier removal.\n",
    "        :param: ds_base: An optional dictionary representing the base dataset (topic -> list of strings). If None, it's loaded using load_raw_data.\n",
    "\n",
    "    Returns:\n",
    "        :return: A tuple containing the filtered base dataset, the filtered translated dataset, and the aggregated dataset.\n",
    "    \"\"\"\n",
    "    if ds_base is None:\n",
    "        ds_base = load_raw_data(language, list(ds_translated.keys()))\n",
    "    # Be sure that we are dealing only with the same topics.\n",
    "    common_keys = set(ds_translated.keys()).intersection(ds_base.keys())\n",
    "\n",
    "    # Due to the \"batched\" dataset (a big chunk of text is divided in multiple parts)\n",
    "    #  we need to be sure that, for each topic, we have the same number of entries.\n",
    "    common_keys = [k for k in common_keys if len(ds_base[k]) == len(ds_translated[k])]\n",
    "\n",
    "    # Filtering both dataset and removing noisy topics\n",
    "    raw_base = {k: ds_base[k] for k in common_keys}\n",
    "    # Don't trust the dataset. Removing entries for which we have wikipedia-related noisy text\n",
    "    raw_base = remove_content_by_target(raw_base, [\"{|\", \"|}\", \"[[\", \"]]\", \"{{\", \"}}\"])\n",
    "\n",
    "    # Remove topics that contain a high number of numbers/punctuation.\n",
    "    # By this way, we can have a more clean dataset\n",
    "    raw_base = clean_dataset(raw_base, n)\n",
    "    raw_translated = {k: ds_translated[k] for k in raw_base}\n",
    "    raw_translated = clean_dataset(raw_translated, n)\n",
    "    raw_base = {k: ds_base[k] for k in raw_translated}\n",
    "\n",
    "    # Verify that we are dealing with the same topics\n",
    "    assert raw_base.keys() == raw_translated.keys()\n",
    "    # Verify that for each topic we have the same number of \"separated batches\"\n",
    "    assert (\n",
    "            sum(\n",
    "                [\n",
    "                    abs(len(ia) - len(it))\n",
    "                    for ia, it in zip(raw_translated.values(), raw_base.values())\n",
    "                ]\n",
    "            )\n",
    "            == 0\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[{language}] Raw entries: {len(ds_translated)} | Clean entries: {len(raw_base)}\"\n",
    "    )\n",
    "    return raw_base, raw_translated, aggregate_dataset(raw_base, raw_translated)\n",
    "\n",
    "\n",
    "raw_base_ia_en, raw_translated_ia_en, ds_aggregated_en = filter_dataset(wikilanguage, ds_overlap_en)\n",
    "raw_base_ia_it, raw_translated_ia_it, ds_aggregated_it = filter_dataset(wikilanguage, ds_overlap_it)\n",
    "DATASET_TYPE_TRANSLATE = \"translate\"\n",
    "DATASET_TYPE_DESCRIBE = \"describe\"\n",
    "DATASET_TYPES = [DATASET_TYPE_TRANSLATE, DATASET_TYPE_DESCRIBE]\n",
    "# We let the possibility to the final user to train against multiple languages\n",
    "all_datasets = [\n",
    "    {\n",
    "        \"starting_language\": starting_language,\n",
    "        \"translated_language\": \"english\",\n",
    "        \"dataset\": ds_aggregated_en,\n",
    "        \"type\": DATASET_TYPES,\n",
    "    },\n",
    "    {\n",
    "        \"starting_language\": starting_language,\n",
    "        \"translated_language\": \"italian\",\n",
    "        \"dataset\": ds_aggregated_it,\n",
    "        \"type\": DATASET_TYPES,\n",
    "    },\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "268fb26e3f1cdf4e",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "Here we are creating the dataset leveraging the prompts above.\n",
    "We also remove the entries for which the prompt + wikitext is above the given number of tokens used for fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0e2b82b842a6e68",
   "metadata": {},
   "source": [
    "ds = pd.DataFrame()\n",
    "for dataset_conf in tqdm(all_datasets):\n",
    "    starting_language = dataset_conf[\"starting_language\"]\n",
    "    translated_language = dataset_conf[\"translated_language\"]\n",
    "    dataset = dataset_conf[\"dataset\"]\n",
    "\n",
    "    # Creating a dataframe from the given \"raw\" dataset.\n",
    "    # The dataset will contain [\"topic\", \"original_content\", \"translated_content\"]\n",
    "    raw_text = []\n",
    "    _ = [\n",
    "        [raw_text.append([k, v[\"original\"], v[\"translated\"]]) for v in vs]\n",
    "        for k, vs in dataset.items()\n",
    "    ]\n",
    "    tdf = pd.DataFrame(raw_text)\n",
    "    tdf.columns = [\"topic\", \"original_content\", \"translated_content\"]\n",
    "\n",
    "    # `ds` will be the dataframe where we store all the training/test data\n",
    "    # In order to increase the quantity of data, we use \"original-to-translated\" and the reverse, \"translated-to-original\".\n",
    "    # By this way, we can increase the quantity of data.Also, we use the original (-> in native language) data to train the network to \"describe\" that topic.\n",
    "    # NOTE: A nice thing to do during training is to organize the dataset to have, for each batch of training data, both \"original-to-translated\" and \"translated-to-original\" examples.\n",
    "    # We are not doing this \"packing\" trick due to the low memory of my GPU.\n",
    "\n",
    "    for dataset_type in dataset_conf[\"type\"]:\n",
    "        if dataset_type == DATASET_TYPE_DESCRIBE:\n",
    "            _tdf = tdf.copy()\n",
    "            # Using the original content (in native language of dataset)\n",
    "            _tdf[\"prompt\"] = _tdf.apply(\n",
    "                lambda x: alpaca_prompt_template.format(\n",
    "                    instruction_describe.format(starting_language),\n",
    "                    x[\"topic\"],\n",
    "                    x[\"original_content\"],\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            _tdf[\"starting_language\"] = translated_language\n",
    "            _tdf[\"translated_language\"] = starting_language\n",
    "            _tdf[\"task_type\"] = DATASET_TYPE_DESCRIBE\n",
    "            ds = pd.concat([ds, _tdf])\n",
    "        elif dataset_type == DATASET_TYPE_TRANSLATE:\n",
    "            # I've simplified the code avoiding fancy method (this is memory inefficient). Also, I want the code to be \"hackable\" to be modified by the final user purpose.\n",
    "            # Here I store the \"original-to-translated\" translation\n",
    "            tdf1 = tdf.copy()\n",
    "            # Here I store the \"translated-to-original\" translation\n",
    "            tdf2 = tdf.copy()\n",
    "\n",
    "            # Translate from ORIGINAL_LANGUAGE --> TRANSLATED_LANGUAGE\n",
    "            tdf1[\"prompt\"] = tdf1.apply(\n",
    "                lambda x: alpaca_prompt_template.format(\n",
    "                    instruction_translate.format(\n",
    "                        starting_language, translated_language\n",
    "                    ),\n",
    "                    x[\"original_content\"],\n",
    "                    x[\"translated_content\"],\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            tdf1[\"starting_language\"] = starting_language\n",
    "            tdf1[\"translated_language\"] = translated_language\n",
    "            tdf1[\"task_type\"] = DATASET_TYPE_TRANSLATE\n",
    "            ds = pd.concat([ds, tdf1])\n",
    "\n",
    "            # Translate from TRANSLATED_LANGUAGE --> ORIGINAL_LANGUAGE\n",
    "            tdf2[\"prompt\"] = tdf2.apply(\n",
    "                lambda x: alpaca_prompt_template.format(\n",
    "                    instruction_translate.format(\n",
    "                        translated_language, starting_language\n",
    "                    ),\n",
    "                    x[\"translated_content\"],\n",
    "                    x[\"original_content\"],\n",
    "                ),\n",
    "                axis=1,\n",
    "            )\n",
    "            tdf2[\"starting_language\"] = translated_language\n",
    "            tdf2[\"translated_language\"] = starting_language\n",
    "            original_content = tdf2.pop(\"original_content\")\n",
    "            translated_content = tdf2.pop(\"translated_content\")\n",
    "            tdf2[\"original_content\"] = translated_content\n",
    "            tdf2[\"translated_content\"] = original_content\n",
    "            tdf2[\"task_type\"] = DATASET_TYPE_TRANSLATE\n",
    "            ds = pd.concat([ds, tdf2])\n",
    "\n",
    "df = ds.drop_duplicates(\"prompt\").reset_index(drop=True)\n",
    "del ds\n",
    "_collect()\n",
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "tokenizer = load_correct_tokenizer(model_id, max_seq_length)\n",
    "EOS_TOKEN = tokenizer.eos_token\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1a04b7d7d2f00126",
   "metadata": {},
   "source": [
    "### Dataset \"metrics\" + cleaning\n",
    "Here I remove the entries for which, after concatenating the prompt, exceed the maximum token sizes\n",
    "Also, I perform some sort of cleaning removing \"strange\" batches of data"
   ]
  },
  {
   "cell_type": "code",
   "id": "88ec3fcc302b546b",
   "metadata": {},
   "source": [
    "df[\"prompt\"] = df[\"prompt\"] + EOS_TOKEN\n",
    "df[\"new_lines_ratio\"] = df[\"prompt\"].apply(lambda x: x.count(\"\\n\") / len(x.split()))\n",
    "df[\"n_tokens\"] = df[\"prompt\"].progress_apply(lambda x: len(tokenizer.encode(x)))\n",
    "df[\"len_ratio\"] = df.apply(\n",
    "    lambda x: len(x[\"original_content\"].split()) / len(x[\"translated_content\"].split()),\n",
    "    axis=1,\n",
    ")\n",
    "mask_ratio = ~find_outliers(df[\"len_ratio\"], n=1)\n",
    "mask_len = (df[\"n_tokens\"] >= min_seq_length) & (df[\"n_tokens\"] <= max_seq_length)\n",
    "mask_lines = ~find_outliers(df[\"new_lines_ratio\"])\n",
    "mask = mask_len & mask_lines & mask_ratio\n",
    "df = df[mask].reset_index(drop=True)\n",
    "df[\"len\"] = df[\"prompt\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\n",
    "    f\"Removing {(~mask).sum()} entries that are out of [{max_seq_length} - {min_seq_length}] tokens. {len(df)} entries remaining.\"\n",
    ")\n",
    "sns.histplot(df[\"len\"], kde=True).set(title=\"Number of words\")\n",
    "plt.show()\n",
    "sns.histplot(df[\"n_tokens\"], kde=True).set(title=\"Number of tokens\")\n",
    "plt.show()\n",
    "sns.histplot(df[\"n_tokens\"] / df[\"len\"], kde=True).set(title=\"Tokens/Word ratio\")\n",
    "plt.show()\n",
    "sns.histplot(df[\"len_ratio\"], kde=True).set(title=\"Len ratio\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76ea4ca5a9a76bac",
   "metadata": {},
   "source": [
    "### Creating the final dataest (train + test)\n",
    "The test set is retrieved using some sort of \"similarity\" between documents.\n",
    "This is because I want to cover as many topics as possible, to let the network have the chance to generalize better\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f476fffd4c2c09ce",
   "metadata": {},
   "source": [
    "# Finding similar documents that share the same languages and task_type (translate, describe)\n",
    "filtered_df = pd.DataFrame()\n",
    "duplicated_df = pd.DataFrame()\n",
    "for group_name, _df in df.groupby(\n",
    "        [\"starting_language\", \"translated_language\", \"task_type\"]\n",
    "):\n",
    "    new_df, dups = remove_similar_documents(\n",
    "        _df, \"original_content\", similarity_threshold=0.2\n",
    "    )\n",
    "    print(\n",
    "        f'Removed {len(_df) - len(new_df)} entries for {dict(zip([\"starting_language\", \"translated_language\", \"task_type\"], list(group_name)))}'\n",
    "    )\n",
    "    filtered_df = pd.concat([filtered_df, new_df], ignore_index=True)\n",
    "    duplicated_df = pd.concat([duplicated_df, dups], ignore_index=True)\n",
    "del df\n",
    "df = filtered_df.reset_index(drop=True).copy()\n",
    "del filtered_df\n",
    "gc.collect()\n",
    "len(duplicated_df) / len(df) * 100\n",
    "display(df.sample(2))\n",
    "df[[\"starting_language\", \"translated_language\", \"task_type\"]].value_counts().sort_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b346fb1573af91c",
   "metadata": {},
   "source": [
    "for group, tdf in df.groupby([\"starting_language\", \"translated_language\", \"task_type\"]):\n",
    "    print(\"=\" * 30, group, \"=\" * 30)\n",
    "    print(tdf[\"prompt\"].sample(1).item())\n",
    "\n",
    "ds_train = df.sample(frac=1, random_state=seed)\n",
    "ds_test = duplicated_df.sample(frac=1, random_state=seed)\n",
    "train_topics = set(ds_train['topic'].unique())\n",
    "test_topics = set(ds_test['topic'].unique())\n",
    "print(\n",
    "    f\"Train examples: {len(ds_train)} | Test Examples: {len(ds_test)} | Common topics: {len(train_topics.intersection(test_topics))} | Only train topics: {len(train_topics - test_topics)} | Only test topics: {len(test_topics - train_topics)}\"\n",
    ")\n",
    "dataset_train = Dataset.from_pandas(ds_train.convert_dtypes())\n",
    "dataset_test = Dataset.from_pandas(ds_test.convert_dtypes())\n",
    "dataset_train.save_to_disk(f\"datasets/{starting_language}_{'_'.join(DATASET_TYPES)}_train.hf\")\n",
    "dataset_test.save_to_disk(f\"datasets/{starting_language}_{'_'.join(DATASET_TYPES)}_test.hf\")\n",
    "for x in dataset_train:\n",
    "    break\n",
    "x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e72bf0c7d83ebbfc",
   "metadata": {},
   "source": [
    "# Another \"dummy\" method for create the test set, is to use the latest batch for each topic. By this way, the dataset will contain some overlapped sentences.\n",
    "# I've choosen to don't use this method but can be useful for play a bit with the dataset creation\n",
    "# def create_dataset(dataset):\n",
    "#     ds_train = []\n",
    "#     ds_test = []\n",
    "#     for topic, data in tqdm(\n",
    "#         dataset.groupby(\n",
    "#             [\"starting_language\", \"translated_language\", \"task_type\", \"topic\"]\n",
    "#         )\n",
    "#     ):\n",
    "#         if len(data) == 1:\n",
    "#             ds_train.extend(data.to_dict(orient=\"records\"))\n",
    "#         else:\n",
    "#             ds_train.extend(data.iloc[:-1].to_dict(orient=\"records\"))\n",
    "#             ds_test.extend(data.iloc[-1:].to_dict(orient=\"records\"))\n",
    "#     return ds_train, ds_test\n",
    "\n",
    "\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df[\"id\"] = df.index\n",
    "# ds_train, ds_test = create_dataset(df)\n",
    "# ds_train = pd.DataFrame(ds_train).sample(frac=1, random_state=seed)\n",
    "# ds_test = pd.DataFrame(ds_test).sample(frac=1, random_state=seed)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ce9ee29eff2cdcc",
   "metadata": {},
   "source": [
    "# Chapter 2: Fine-tune the Gemma2 model leveraging Unsloth\n",
    "A `json` file is delegated to save the configuration of the model that should be fine-tuned for reproducibility.\n",
    "I've manually iterated the hyperparameter configurations (unfortunately, it was impossible to leverage optuna for this type of task with my current machine configuration).\n",
    "I've found that the best params are the ones present in the `configs` folder.\n",
    "Refer to `tensorboard` plots to have an idea.\n",
    "\n",
    "For have a clear understanding about what is happening, please refer to the following [guide](https://colab.research.google.com/github/google-gemini/gemma-cookbook/blob/main/Gemma/Finetune_with_Unsloth.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "id": "310398a29fc4027",
   "metadata": {},
   "source": [
    "training_config = \"configs/config_3.3-candidate-2epochs-ia.json\"\n",
    "config = json.load(open(training_config))\n",
    "pprint(config)\n",
    "\n",
    "modules_to_save = config.get(\"modules_to_save\")\n",
    "dataset_name = config.get(\"dataset_name\")\n",
    "epochs = config.get(\"epochs\")\n",
    "model_id = config.get(\"model_id\")\n",
    "learning_rate = config.get(\"learning_rate\")\n",
    "lr_scheduler_type = config.get(\"lr_scheduler_type\")\n",
    "r = config.get(\"r\")\n",
    "dropout = config.get(\"dropout\")\n",
    "lora_alpha = config.get(\"lora_alpha\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5284762f885f2f89",
   "metadata": {},
   "source": [
    "def load_base_model(model_id, max_seq_length, device=\"sequential\"):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "        device_map=device,\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "model, tokenizer = load_base_model(model_id, max_seq_length)\n",
    "\n",
    "dataset_train = Dataset.load_from_disk(f\"datasets/{dataset_name}_train.hf\")\n",
    "dataset_test = Dataset.load_from_disk(f\"datasets/{dataset_name}_test.hf\")\n",
    "# Removing the \"describe\" dataset from the test set.\n",
    "ds = dataset_test.to_pandas()\n",
    "ds.pop('__index_level_0__')\n",
    "ds = ds[ds[\"task_type\"] == \"translate\"]\n",
    "dataset_test = Dataset.from_pandas(ds)\n",
    "del ds\n",
    "_collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "603d5089cf31e55",
   "metadata": {},
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=dropout,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=seed,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    "    modules_to_save=modules_to_save,\n",
    ")\n",
    "\n",
    "do_eval = True  #'9b' not in model_id\n",
    "batch_size = 2  # if do_eval else 1\n",
    "grad_acc_batch_size = 8  # if do_eval else 16\n",
    "training_args = TrainingArguments(\n",
    "    # auto_find_batch_size=not do_eval,\n",
    "    do_train=True,\n",
    "    do_eval=do_eval,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=1 if do_eval else None,\n",
    "    gradient_accumulation_steps=grad_acc_batch_size,\n",
    "    eval_accumulation_steps=grad_acc_batch_size if do_eval else None,\n",
    "    eval_strategy=\"steps\" if do_eval else 'no',\n",
    "    # Perform eval at each 10% of training dataset\n",
    "    eval_steps=0.1 if do_eval else 0,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    num_train_epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    seed=seed,\n",
    "    output_dir=\"outputs\",\n",
    "    report_to=[\"tensorboard\"],\n",
    ")\n",
    "\n",
    "if not do_eval: dataset_test = None\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "dl = trainer.get_train_dataloader()\n",
    "# Ensure that the tokenizer respects the max length\n",
    "for batch in dl:\n",
    "    assert len(batch[\"input_ids\"][0]) <= max_seq_length\n",
    "    print(tokenizer.decode(batch[\"input_ids\"][0]))\n",
    "    del batch\n",
    "    del dl\n",
    "    break"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba7bc991a2c3232f",
   "metadata": {},
   "source": [
    "### Save the model asset"
   ]
  },
  {
   "cell_type": "code",
   "id": "338f0bc2911bb96c",
   "metadata": {},
   "source": [
    "_collect()\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "new_model = f\"models/{os.path.basename(model_id)}_unsloth_{dataset_name}_{os.path.basename(os.path.splitext(os.path.basename(training_config))[0])}\"\n",
    "model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "print(new_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf178b339d02abde",
   "metadata": {},
   "source": [
    "# Chapter 3: Inference and evaluation\n",
    "In order to evaluate the mode, with let the user to choose between [BLEU](https://en.wikipedia.org/wiki/BLEU) or [GLEU](https://oecd.ai/en/catalogue/metrics/google-bleu-gleu)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sentence_metric(translated: str, original: str, metric=gleu) -> float:\n",
    "    \"\"\"Calculates a sentence-level metric score between a translated and an original sentence.\n",
    "\n",
    "    Args:\n",
    "        :param: translated: The translated sentence (string).\n",
    "        :param: original: The original sentence (string).\n",
    "        :param: metric: The metric function to use {gleu, bleu}.\n",
    "\n",
    "    Returns:\n",
    "        :return: The sentence-level metric score (float).\n",
    "    \"\"\"\n",
    "    # Get the name of the sentence-level scoring method from the metric object\n",
    "    method_name = [name for name in metric.__dict__.keys() if \"sentence\" in name][0]\n",
    "    score_method = getattr(metric, method_name)\n",
    "    return score_method([translated.split()], original.split())\n",
    "\n",
    "\n",
    "def corpus_metric(translated: List[str], original: List[str], metric=gleu) -> float:\n",
    "    \"\"\"Calculates a corpus-level metric score between a list of translated sentences and a list of original sentences.\n",
    "\n",
    "    Args:\n",
    "        :param: translated: A list of translated sentences (strings).\n",
    "        :param: original: A list of original sentences (strings).\n",
    "        :param: metric: The metric function to use {gleu, bleu}.\n",
    "\n",
    "    Returns:\n",
    "        :return: The corpus-level metric score (float).\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the lengths of the translated and original lists are not equal.\n",
    "    \"\"\"\n",
    "    assert len(translated) == len(original)\n",
    "    translated = [[sentence.split()] for sentence in translated]\n",
    "    original = [sentence.split() for sentence in original]\n",
    "\n",
    "    # Get the name of the corpus-level scoring method from the metric object.\n",
    "    method_name = [name for name in metric.__dict__.keys() if \"corpus\" in name][0]\n",
    "    score_method = getattr(metric, method_name)\n",
    "    return score_method(translated, original)\n",
    "\n",
    "\n",
    "def calculate_metrics(references: List[str], hypothesis: List[str], metric=gleu):\n",
    "    \"\"\"Calculates sentence-level and corpus-level metrics between a list of reference sentences and a list of hypothesis (translated) sentences.\n",
    "\n",
    "    Args:\n",
    "        :param: references: A list of reference sentences (strings).\n",
    "        :param: hypothesis: A list of hypothesis (translated) sentences (strings).\n",
    "        :param: metric: The metric function to use {bleu, gleu}.\n",
    "\n",
    "    Returns:\n",
    "        :return: A tuple containing the mean sentence-level metric score (float) and the corpus-level metric score (float).\n",
    "    \"\"\"\n",
    "    _results: List[Tuple[str, str, float]] = []\n",
    "    for test_example, model_response in zip(references, hypothesis):\n",
    "        m = sentence_metric(model_response, test_example, metric)\n",
    "        _results.append((model_response, test_example, m))\n",
    "    return np.mean([v[2] for v in _results]).item(), corpus_metric([v[0] for v in _results], [v[1] for v in _results])\n",
    "\n",
    "\n",
    "hyp = \"she read the book because she was interested in world history\"\n",
    "ref_a = \"she read the book because she was interested in world history\"\n",
    "ref_b = \"she was interested in world history because she read the book\"\n",
    "sentence_metric(ref_b, hyp), corpus_metric([ref_b, ref_b], [hyp, hyp]), sentence_metric(\n",
    "    ref_b, hyp, bleu\n",
    "), corpus_metric([ref_b, ref_b], [hyp, hyp], bleu)"
   ],
   "id": "66e284661dc26033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create the inference dataset, leveraging the input prompt and the `apply_chat_template` method from the tokenizer",
   "id": "7d55c4dd5d81423e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"dataset\": f\"datasets/{starting_language}_{'_'.join(DATASET_TYPES)}_test.hf\",\n",
    "    \"model\": new_model,\n",
    "\n",
    "}\n",
    "model, _ = load_base_model(\n",
    "    config['model'],\n",
    "    max_seq_length,\n",
    ")\n",
    "# No matter the model used, we use the same dataset-format (with standard chat template)\n",
    "tokenizer = load_correct_tokenizer(\"google/gemma-2-2b-it\")\n",
    "tokenizer.pad_token = '<pad>'\n",
    "tokenizer.eos_token = '<eos>'\n",
    "MODEL_START_TOKEN = \"<start_of_turn>model\"\n",
    "FastLanguageModel.for_inference(model)\n",
    "_collect()\n",
    "\n",
    "\n",
    "def tokenize_for_inference(entries: List[str], tokenizer=tokenizer, to_gpu=False):\n",
    "    tokenized = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": entry}], return_tensors=\"pt\"\n",
    "        )\n",
    "        for entry in entries\n",
    "    ]\n",
    "    if to_gpu:\n",
    "        tokenized = [entry.to(\"cuda\") for entry in tokenized]\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def extract_response(model_response: str):\n",
    "    n = model_response.find(MODEL_START_TOKEN)\n",
    "    response = (\n",
    "        model_response[n:]\n",
    "        .replace(\"### Response:\\n\", \"\")\n",
    "        .removeprefix(MODEL_START_TOKEN)\n",
    "        .removesuffix(\"<end_of_turn>\")\n",
    "        .strip()\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def create_inference_dataset(original_content, translated_content, ids, to_gpu=True):\n",
    "    return (\n",
    "        tokenize_for_inference(original_content, to_gpu=to_gpu),\n",
    "        translated_content,\n",
    "        ids,\n",
    "    )\n",
    "\n",
    "\n",
    "df_test = Dataset.load_from_disk(config['dataset']).to_pandas()\n",
    "# Using only the `translate-related` dataset\n",
    "df_test = df_test[df_test[\"task_type\"] == \"translate\"]\n",
    "pprint(df_test.sample(1).to_dict(orient=\"records\"))\n",
    "# Create an id based on the dataset columns\n",
    "df_test[\"hash\"] = df_test.apply(\n",
    "    lambda x: hash(\n",
    "        x[\"topic\"]\n",
    "        + \"|\"\n",
    "        + x[\"original_content\"]\n",
    "        + \"|\"\n",
    "        + x[\"translated_content\"]\n",
    "        + \"|\"\n",
    "        + x[\"starting_language\"]\n",
    "        + \"|\"\n",
    "        + x[\"translated_language\"]\n",
    "        + \"|\"\n",
    "        + x[\"task_type\"]\n",
    "        + \"|\"\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# For the inference phase, I use the `apply_chat_template` for compatibility, so we clean the template from the manually set special tokens.\n",
    "alpaca_prompt_template_inference = alpaca_prompt_template.replace(\"<start_of_turn>user\", \"\").replace(\n",
    "    \"<start_of_turn>model\", \"\").replace(\"<end_of_turn>\", \"\")\n",
    "\n",
    "df_test[\"question\"] = df_test.apply(\n",
    "    lambda row: alpaca_prompt_template_inference.format(\n",
    "        instruction_translate.format(\n",
    "            row[\"starting_language\"], row[\"translated_language\"]\n",
    "        ),\n",
    "        row[\"original_content\"],\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "print(df_test[\"question\"].sample(1).item())\n",
    "df_test.rename(columns={\"translated_content\": \"answer\"}, inplace=True)\n",
    "df_test = df_test[[\"topic\", \"question\", \"answer\", \"starting_language\", \"hash\"]]\n",
    "\n",
    "test_datasets = {}\n",
    "for group, _df in df_test.groupby(\"starting_language\"):\n",
    "    test_datasets[group] = _df.copy()\n",
    "for k, v in test_datasets.items():\n",
    "    print(f\"{k} -> {len(v)}\")\n",
    "\n",
    "example_in_sentence = (\n",
    "    test_datasets[\"italian\"].sample(1, random_state=43)[\"question\"].item()\n",
    ")\n",
    "example_out_sentence = (\n",
    "    test_datasets[\"italian\"].sample(1, random_state=43)[\"answer\"].item()\n",
    ")\n",
    "print(example_in_sentence, \"\\n-------\\n\", example_out_sentence)\n"
   ],
   "id": "854f5f6ff9cf695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### An example of response",
   "id": "52e8084ff45abb4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "example_tokenized_sentence = tokenizer.apply_chat_template(\n",
    "    [{\"role\": \"user\", \"content\": example_in_sentence}], return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    example_model_res = model.generate(example_tokenized_sentence).detach().cpu()\n",
    "    # print(tokenizer.decode(example_model_res[0], skip_special_tokens=True))\n",
    "detokenized_example_model_res = tokenizer.decode(\n",
    "    example_model_res[0][len(example_tokenized_sentence[0]):], skip_special_tokens=True\n",
    ").strip()\n",
    "print(tokenizer.batch_decode(example_tokenized_sentence)[0])\n",
    "print(detokenized_example_model_res)\n",
    "print(\"---\" * 30)\n",
    "print(example_out_sentence)"
   ],
   "id": "7882780438864765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Packing the dataset",
   "id": "c804890d1a58c862"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inference_datasets = {}\n",
    "for k, ds in test_datasets.items():\n",
    "    inputs, outputs, ids = create_inference_dataset(\n",
    "        ds[\"question\"].to_list(),\n",
    "        ds[\"answer\"].to_list(),\n",
    "        ds[\"hash\"].to_list(),\n",
    "    )\n",
    "    inference_datasets[k] = {\"inputs\": inputs, \"outputs\": outputs, \"ids\": ids}\n",
    "inputs, outputs = (\n",
    "    inference_datasets[\"italian\"][\"inputs\"],\n",
    "    inference_datasets[\"italian\"][\"outputs\"],\n",
    ")\n",
    "for input_entry, output_entry in zip(inputs, outputs):\n",
    "    print(tokenizer.batch_decode(input_entry, skip_special_tokens=False)[0])\n",
    "    print(\"---\" * 30)\n",
    "    print(output_entry)\n",
    "    print(\"===\" * 30)\n",
    "    break\n",
    "_collect()"
   ],
   "id": "6c2ed6cf71a0eba2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "generation_config = {\"max_new_tokens\": max_seq_length} | {\n",
    "    # \"do_sample\": True,\n",
    "    # \"early_stopping\": True,\n",
    "    # \"temperature\": 0.1,\n",
    "    # \"top_k\": 10,\n",
    "    # \"top_p\": 0.1,\n",
    "    # \"max_new_tokens\": 256,\n",
    "    # \"repetition_penalty\": 1.3,\n",
    "}\n",
    "# with torch.inference_mode():\n",
    "with torch.no_grad():\n",
    "    for language in inference_datasets:\n",
    "        inference_dataset = inference_datasets[language]\n",
    "        inference_dataset[\"response\"] = []\n",
    "        for _input in tqdm(inference_dataset[\"inputs\"], desc=language):\n",
    "            inference_dataset[\"response\"].append(\n",
    "                model.generate(_input, **generation_config).detach().cpu()\n",
    "            )\n",
    "\n",
    "pickle.dump(\n",
    "    inference_datasets, open(f\"inference_{os.path.basename(config['dataset'])}.dict.pkl\", \"wb\")\n",
    ")\n",
    "\n",
    "for language in inference_datasets:\n",
    "    ds_response = inference_datasets[language]\n",
    "    break\n",
    "ds_response.keys()\n",
    "detokenized_responses = []\n",
    "for i in range(len(ds_response[\"response\"])):\n",
    "    _input = ds_response[\"inputs\"][i]\n",
    "    answer = ds_response[\"response\"][i]\n",
    "    _res = tokenizer.decode(\n",
    "        answer[0][len(_input[0]):], skip_special_tokens=False\n",
    "    ).strip()\n",
    "    detokenized_responses.append(extract_response(_res))\n",
    "ds_response['answer'] = detokenized_responses\n",
    "import numpy as np\n",
    "\n",
    "calculate_metrics(ds_response[\"answer\"], ds_response[\"outputs\"])"
   ],
   "id": "a3af9312bd49ae2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
